\documentclass[main.tex]{subfiles}

\begin{document}

\subsection{Initial \gama Hands-On}

While studying the details of the framework, as well as the theoretical foundations of the progressive photon mapping case study, some tests were already done with \gama. Initially some of the source code of the included samples, such as the SAXPY and Barnes-Hut algorithms were studied.

Later, an implementation of a first order finite volume method was implemented using GAMA, using the previously implemented versions of that same algorithm as basis of comparison. This versions included a sequential implementation, and two parallelized implementations, one with \openmp, and another with \cuda. The details of the algorithm are described in more detail in \cref{sec:polu}

\subsubsection{The Polu Case-Study} \label{sec:polu}

The application, here called \texttt{polu}, was already the subject of a parallelization study in \cite{naps2012}, which described the incremental work where the application was improved from a sequential implementation, first through a process of analysis and sequential optimization, and then subject to parallelization using two techniques, a shared memory \cpu implementation with \openmp, and a \gpu implementation with \cuda. A third implementation stage focused on the development of a distributed memory implementation using \acs{MPI}, but without providing any positive performance results.

The \texttt{polu} application, computes the spread of a material (e.g.\ a pollutant) in a bi-dimensional surface through the course of time. This surface is discretely represented as a mesh, composed mainly of edges and cells. The input data set contains information about the mesh description, the velocity vector for each cell and an initial pollution distribution.
Both inputs and outputs are compatible with the \texttt{gmsh} application, for data visualization.

\subsubsubsection{The algorithm}

The algorithm used by this application is a first order finite volume method. This means that each mesh element only communicates directly with its first level neighbours in the mesh, which makes this a typical case of a stencil computation. In terms of performance, being a stencil algorithm implies that the operational intensity will most likely remain constant with larger problem sizes \cite{williams2009roofline,williams2010roofline}. Despite this, the algorithm is still very irregular in terms of memory access patterns, because meshes generated by \texttt{gmsh} suffered from deep locality issues, turning memory accesses ordered by cells or edges close to random.

The execution of the algorithm consists of looping through two main kernels, advancing in time until an input-defined limit is reached. These two kernels are:
\begin{description}
\item[\texttt{compute\_flux}] \hfill \\
  In this step, a flux is calculated for each edge, based on the current pollution concentration of each of the adjacent cells. A constanct known as \text{Dirichlet} condition is used for the boundary edges of the mesh, replacing the value of the missing cell. This flux value represents the amount of pollution that travels across that edge in that time step.

\item[\texttt{update}] \hfill \\
  With the previously calculated fluxes, all cell values are updated, with each cell receiving contributions from all the adjacent edges. After this, one time step has passed.
\end{description}

\subsubsubsection{Implementation}

In order to run the algorithm using the framework, both kernels had to be re-implemented using \gama jobs. Additionally, data structures had to be re-written to make use of the facilities provided by \gama to allow memory to be automatically handled by the global address space.
This presents an obviously large amount of development work, since mostly everything had to be re-written according to \gama rules. However, it has to be taken into account the fact that this additional work also had to be performed in the previous implementations studied in \cite{naps2012}, since most of the original code was not suitable for efficient parallelization.

From this, one initial consideration can already be made about the framework, in the sense that the effort required to parallelize it might be too high if a given application is already written with some concerns regarding parallelization (although without \gama). Since specific data structures and job definitions need to be used, this may hamper the adoption of \gama by already implemented solutions, unless the performance advantages are significant enough to justify the additional effort.


\subsubsubsection{Study limitations}

Unfortunately, there are several restrictions to the input generation for this algorithm. In particular, the utility required to generate a mesh with arbitrary resolution has an estimated complexity of $O(N^3)$ which prevented large enough test cases to be generated. The largest available input contained only around $400,000$ cells, and represented a total memory footprint of just over $40MB$, which is extremely small, and does not allow a good enough analysis on resource usage. With such a low resource occupancy, the scheduling policy employed by \gama will most likely assign all the workload to a single device, as the cost of data transfers, and the low execution time for each kernel for such a small data set would not justify otherwise. Additionally, this being a typical stencil, each iteration requires a barrier, allowing no execution of two simultaneous iterations, which would be an additional way of improving parallelism.

Knowing this, any result obtained by profiling the \texttt{polu} application under these conditions would not provide a correct insight about the algorithm, or about the framework, and as such, these results are not presented here.
The \texttt{polu} test case still served as an initial basis to gain some insight into \gama, and to better prepare the implementation of the progressive photon mapping case study.


\end{document}
