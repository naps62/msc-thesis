\documentclass[main.tex]{subfiles}
\begin{document}

\section{Software}

Several technologies already exist to assist in the production of parallel code. These softwares range from low level drivers, which are sometimes required to access hardware-level features (e.g.\ \cuda or \acs{OpenCL}, which acts as a bridge between the programmer and the \gpu driver) to fully features libraries that attempt to provide higher level directives to developers, providing more flexibility to work on the algorithm itself rather than on hardware specific details and optimizations, which are sometimes hidden away.

Most of these libraries however, limit themselves to a specific set, such as shared memory systems like multi-core \cpus\xspace. This section presents an overview on some of the existing software available for programming in parallel environments. While some the examples presented here may not be directly used throughout the dissertation, it should prove useful to analyze different software approaches, and understand where exactly can \acs{GAMA} prove itself useful.


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\openmp}

\openmp can be described as a set of compiler directives and routines applicable to Fortran and C/C+ code in order to express shared memory parallelism \cite{dagum1998openmp}. It provides a high-level API with a wide range of processors supporting it.
Using the defined directives, the programmer is able to describe how section of an algorithm should be parallelized without introducing too much additional complexity within the code. The programmer is only left with the task of ensuring there are no data races or dependencies within a parallel task. More advanced options like specifying task granularity, private variables, or explicit barriers is also possible, enabling more control over the parallel region of the code.

This has been one of the standards for parallelization on \textit{86} \cpus, including the recent \intel \mic architecture.


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\cuda}

The platform created by \nvidia to be used along with their \gpus. The complete \cuda toolkit provides a driver, a compiler and a set of tools and libraries to assist in the development of highly parallel \gpu code in either C/C++ or Fortran. Wrappers for other languages are also available, as a result of third party work.

\cuda programmers are usually required to pay attention to architectural details of their code, in order to best take advantage of the platform. Unlike OpenMP for instance, where the parallelism is mostly abstracted away from the developer, in \cuda one has to consider the correct use of the available resources, and how to structure the algorithm to fit them. As a result, some higher level wrappers are starting to emerge (e.g.\ \openacc). Despite the added complexity, \cuda has been used extensively in scientific fields such as computational biology, cryptography and many others \cite{manavski2008cuda,vasiliadis2008gnort,manavski2007cuda}



%%%%%%%%%%%%%%%%%%%%%%
\subsection{\mpi}

The \acf{MPI} is a specification for a language-independent communication protocol, used in the parallelization of programs with a distributed memory paradigm. This standard defines the rules of a library that enables message passing capabilities, allowing programs to spawn several processes, and communicate between each other by sending and receiving messages. The processes created can be physically located on different machines (hosts), with the communication being made across the available network connections. The main goals of \mpi are towards high performance, scalability and portability, and is one of the most dominant parallelization models in current high performance computing \cite{sur2006high}.


%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\openacc}
\openacc aims to to provide a standard for programming parallel devices, and simplify the programming of heterogeneous systems.
The programming model introduced by this standard is similar in many ways to \openmp, especially in the usage of compiler directives to specify the parallelism rules. Unlike \acs{OpenMP} however, code produced by \openacc may also be targeted at different devices, such as \gpus, and more recently the new \intel Xeon Phi \cite{openacc-phi2012}.

Currently the major disadvantage of \acs{OpenACC} is the lack of compiler support, with only commercial solutions providing compatibility with it.


%%%%%%%%%%%%%%%%%%%
\subsection{Galois}

The Galois project is currently being developed by the Inteligent Software Systems group, in the University of Austin, Texas \todo{is this right?}, and aims to create a system that automatically parallelizes serial code in shared memory machines, effectivelly making parallel computing easier to write.
The approach used by Galois uses speculative execution and amorphous data-parallelism, which is exploitable even in data structures based mostly on pointers, such as trees or graphs \cite{pingaliamorphous,pingali2011tao}. It provides a set of data structures, classes and utilities that allow the programmer to create `Galoized` code, capable of achieving good parallelization results. Like \gama, Galois has a particular emphasis on irregular algorithms, and requires explicit usage of the provided functions and data structures. However, it focuses more on the algorithmic issues of irregular applications, instead of the scheduling issues inherent of a \acs{HetPlat}.


\end{document}
