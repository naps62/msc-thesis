\documentclass[main.tex]{subfiles}

\begin{document}

\subsection{StarPU} \label{section:impl_starpu}

For the final implementation, using the \starpu framework, most of the remaining work consisted on reusing the existing code for each computational tasks, and submit them as tasks to be scheduled by \starpu. The development process of the previous implementations (\cpu and \cuda) resulted in a very modular solution, with very little coupling\footnote{the degree of dependency between the multiple modules of a system. Tight coupling tends to difficult refactoring of one module without requiring subsequent changes to dependant modules, difficulting the reusability of code} between tasks implementation and the algorithm and scheduling code being used.

\subsubsection{Early Decisions}

An early decision for this implementation was to consider only the low-level API, and not the \texttt{pragma}-based one (see \cref{section:starpu_api}). The reason for this is due to the high-level version being an earlier product, still being in earlier development stages, and not being fully capable of providing the full set of features of \starpu.

Additionally, the actual documentation for the framework is almost entirely focused on the low-level functions, making it easier getting up to speed and understand its usage.

An additional decision that was enforced by the existing implementations is related to the task scheduling pocily used. Since the \cpu implementation of all parallellizable tasks was based on \openmp, it was intuitive to approach the problem by using the capabilities of parallel tasks and combined workers of \starpu (see \cref{section:starpu_multithreading}). The only drawback that comes from this is that only the parallel-aware task schedulers (\texttt{pheft} and \texttt{peager}) are compatible

\subsubsection{Data Management}

The first step for this implementation was to refactor data management, letting \starpu handle all necessary data for the algorithm. One exception was made to this, regarding the input information for the 3D scene. This information is stored in a somewhat complex structure, contrarily to all other dynamic data used throughout the photon mapping algorithm, which consists only on vectors whose size can be static and predetermined.

\starpu's memory management library proved to be only capable of easily managing simple data structures, such as vectors or matrixes, but failed to provide a transparent (and well-documented) way of handling more irregular data structures, where the total size may not be available or determinable, and may even be variable.

The straightforward solution to this was to keep scene information statically stored just as in the previous \cpu and \cuda implementations. Since this structure is completely read-only throughout the entire rendering process, no extra care was required in terms of memory consistency. The scene was statically copied to the main memory, and to the memory of each \cuda device available allowing for each device to freely access its internal copy.

Another small change happened to the lookup table build process. This task previously generated a dynamically sized structure, since it is dependent on the amount of photons that intersect the scene within the current radius of each hit point, which cannot be predetermined.

One solution for this would be to only register the lookup table in a \starpu data handle only after it's generation is complete, and the size can be determined. This is not a desirable solution, as it would introduce a barrier on that point of the iteration, and forcing all future tasks to be submitted only once the lookup table build process is complete. This would prevent \starpu from having knowledge on future tasks, and impeding it from asynchronously prepare data buffers to solve dependencies, increasing the latency caused by the imposed barrier.

This seems to be a rather harsh limitation of \starpu, as irregular sized structures are commonly used. However, an alternative solution is possible for this specific problem. The cell size of the hash grid used is based on the photon radius for the current iteration, in such a way that a cell will never have a width, height or depth greater than the current radius. With that follows that any given hit point will always intersect at most 8 cells. Thus, it can be determined that the maximum hash table size can be set as $8 * \#hit\_points$, for any iteration.

Following this constraint, the hash table structure was refactored to be a fixed-size one, allowing \starpu to handle it without the need for barrier, and allowing future tasks to be submitted at will, using the lookup table data handle as a regular data buffer and dependency.

\subsubsection{Task Submission}

\itodo{talk about the X iterations at once}

\end{document}
