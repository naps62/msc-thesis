\documentclass[main.tex]{subfiles}
\begin{document}

\chapter{Introduction}

\section{Contextualization}

\itodo{evolution of parallel systems}

\itodo{Contextualization - refs refs refs}
Heterogeneous platforms are increasingly popular for high performance computing, with an increasing number of supercomputers taking advantage of accelerating devices in addition to the already powerful tradicional \cpus, to provide higher performance at lower costs. These accelerators are not as general-purpose as a \cpu, but have characteristics that make them more suitable to specific, usually highly parallel tasks, and as such are useful as co-processors that complement the work of conventional systems.

Moore's law \cite{moore1965cramming,moore1975progress} predicted in 19755 that the performance of microprocessors would double every two year. That expectation has driven development of microprocessors until very recently. The number of transistors, and high clock frequencies of today's microprocessors is near the limit of power density, introducing problems such as heat dissipation and power consumption. Facing this limitations, research focus was driven towards multicore solutions.

This marked the beginning of the multi-core era. While multicore systems were already a reality, it was not until this point that they reached mainstream production, and parallel paradigms began to emerge as more general-purpose solutions.

In addition to regular \cpus, other types of devices also emerged as good computational alternatives. In particular, the first \gpus supporting general purpose computing were introduced by \nvidia around the year 2000 \cite{luebke2006gpgpu} \itodo{verificar esta ref}.

These devices graduall evolved from specific hardware dedicated to graphics rendering, to fully featured general programming devices, capable of massive data parallelism and performance, at lower power consumptions.
They enable the acceleration of highly parallel tasks, being more efficient than \cpus in various scientific fields, but also more specialized. The usage of \gpus for general computing has been named \ac{GPGPU}, and has since become an industry standard.
As of 2013, over 50 of the \footnote{A list of the most powerful supercomputers in the world, updated twice a year (\url{http://www.top500.org/})}{TOP500's} list were powered by \gpus, which indicates an exponential growth in usage when compared to previous years. This increased usage is motivated by the effectiveness of these devices for general-purpose computing.

Other types of accelerators recently emerged, like the recent Intel \mic architecture, and while all of them differ from the traditional \cpu architecture, they also differ between themselves, providing different hardware specifications, along with different memory and programming models. \todo{falar mais do mic aqui}

Development of applications targeting these devices tends to be harder, or at least different from conventional programming. One has to take into account the differences of the underlying architecture, as well as the programming model being used, in order to produce code that is not only correct, but also efficient. And efficiency for one device might have a different set of requirements or rules that are inadequate to a different device. As a result, developers code have to take into account the characteristics of each different device they are using within their applications, if they want to fully take advantage of them. Usually, the task of producing the most efficient code for a single platform is a time consuming task, and requires a deep knowledge of the architecture itself, In addition, the parallel nature of these accelerators introduces yet another difficulty layer for developers.

Each accelerator can also be programmed in a variety of ways, ranging from low level programming models such as \cuda or \opencl to higher level libraries like \openmp or \openacc. Each of these provides a different method of writing parallel programs, and has a different level of abstraction about the underlying architecture.

The complexity increases even further when it is considered that multiple accelerators might be used simultaneously. This agravates the already existing problems concerning workload, scheduling, and communication.

\todo{falar aqui de que talvez os aceleradores nao sejam `a cena`?}

Today, these different types of devices are most commonly used (in the context of general-computing) as accelerators, in a system where at least one \cpu manages the main execution flow, and delegates specific tasks to the remaining computing resources. A system that uses different computational units is commonly referred to as a heterogeneous platform, here referred to as a \hetplat. These types of systems are becoming particularly noticeable, as can be seen byt the TOP500 ranking, where an increasing number of top-rated systems are heterogeneous.

Much like the phenomenom seem at the start of the multi-core era, a new paradigm shift must happen in order to correctly use a \hetplat. An even greater level of complexity is introduced, because one has to consider not only the multiple different architectures and programming models being used, but also the distribution of both work and data. A \hetplat is by definition a distributed system, since each device usually has its own memory hierarchy. As much as a given task may be fast on a given device, the data transfers required to offload such task may add an undesirable latency to the process, and can actually be one the performance bottlenecks of this strategy.

Even within a single device, memory hierarchy usage can have a big impact in performance. In a acs{NUMA} system, although each socket can access all memory nodes transparently, access times will be dependent on where the requested data is pinned. Performance problems arise from this if one considers the multiple sockets as one single multicore \cpu. Instead, the topology of the system can be considered when assigning tasks to each individual processing unit, and data transfered accordingly, to avoid expensive memory transactions.

Code efficiency is also becoming extremely volatile, as each new system that emerges usually requires architecture-specific optimizations, rendering previous code obsolete with respect to performance. There is an increasing need for a unified solution that allows developers to keep focuses on the algorithmic issues, and automatize these platform-specific issues, which present a barrier to the development of code targeting \hetplats.

Several frameworks have been developed since around 2008 to target these issues, and allow developers to abstract themselves from the underlying system. These frameworks usually manage the multiple resources of the system, treating both \cpus and co-processors as generic computational devices, able to execute tasks, and employ a scheduler to efficiently distribute workload. Memory management is also a key factor, with memory transfers playing a huge role in today's co-processor efficienty.

Some of these frameworks include \gama \todo{ref}, \starpu \todo{ref}, Qilin \todo{ref}, and the more recent MDR \todo{ref}.

These frameworks tend to encapsulate work by providing the concept of task, which is usually not present in the underlying programming models of the programming languages used, and data dependencies, and employ a task scheduler to assign the existing workload to the available resources.
The scheduler is considered one of the key features of these frameworks. It may takes into account multiple different factors in order to decide when and where to run the submitted tasks. These factors can range from the architectural details of the detected resources, to the measured performance of each task on each device, which can be done by building a history based on previous executions.

\subfile{tex/110_motivation}
\subfile{tex/120_organization}

\end{document}
