\documentclass[main.tex]{subfiles}

\begin{document}

\chapter{The StarPU Framework} \label{chapter:starpu}


\itodo{falar de todo o tipo de coisas aqui}

\starpu \cite{augonnet2011starpu} is a unified runtime system consisting on both software and a runtime API that aims to allow programmers of computational intensive applications to more easily exploit the power of avaiable devices, supporting \cpus, \gpus and Cell \todo{ref aqui para o cell? ou subsection algures a falar dele talvez}.

Much like \gama, this framework frees the programmer of the workload scheduling and data consistency inherent from a \hetplat. Task submissions are handled by the \starpu task scheduler, and data consistency is ensured via a data managemente library.

\subsection{Terminology}
\itodo{this}

\starpu uses a well defined terminology to describe its libraries and API:

\begin{description}
  \item[Data Handle] References a memory block. The allocation of the required space, and the possibly required memory transfers to deliver information to each device can be completely handled by \starpu;

  \item[Codelet] Describes a computational kernel that can be implemented in one or more architectures, such as \cpus, \cuda or \acs{OpenCL}. It also stores information about thew amount and type of data buffers it should receive;

  \item[Task] Is defined as the association between a codelet and a set of data handles;

  \item[Partition] The subdivision of a data handle in smaller chunks, according to a partitioning function, which can be user defined;

  \item[Worker] A processing element, such as a \cpu core, managed by StarPU to execute tasks;

  \item[Scheduler] The library in charge of assigning tasks to workers, based on a well defined scheduling policy.

\end{description}

\section{Task Scheduling}
\itodo{this}

The framework employs a task based programming model. Computational kernels must be encapsulated within a task. A given kernel can be implemented in multiple ways (i.e. for \cpus or for \cuda), and \starpu will handle the decision of where and when the task should be executed, based on a task scheduling policy.

Data manipulated by a task is automatically transferred as needed between the various processing devices, ensuring memory consistency and freeing the programmer from dealing directly with scheduling issues, data transfers and other requirements associated with it.

\section{Dependencies}
\itodo{this}

\starpu automatically buils a dependency graph of all submitted tasks, and keeps them in a pool of \emph{frozen tasks}, passing them onto the scheduler once all dependencies are met.

Dependencies can be implicitly given by the data manipulated by the task. Each task receives a set of buffers, each one corresponding to a piece of data managed by \starpu data management library, and will wait until all the buffers from which it must read are ready.

This includes the possible data transfers that are required to meet dependencies, in case different tasks that depend on the same data are scheduled to run on different computational nodes. \starpu will automatically make sure the required data transfers are made between each task exectution to ensure data consistency.

In addition to implicit data dependencies, other dependencies can be explicitly given by using the \starpu API in order to explicitly force the execution order of a given set of tasks.

\subsection{Data Access Modes}

Each data dependency that is explicitly defined in a task can have a different access mode. Data can be used in read-only, write-only or read-write mode. This access mode does not serve the purpose of ensuring memory correctness. It is used to soften task dependencies by using a \emph{Multiple Readers / Single Writer} model in dependency calculation.

This model describes a type of mutual exclusion pattern where a data block can be concurrently accessed by any amount of reader, but must be exclusively accessed for a writer. \starpu uses this concept to further optimize data dependency calculations. If multiple scheduled tasks depend on the same data handle, but only with reading access, then that dependency should not block the multiple tasks from running concurrently. Temporary copies of the data can be created, possibly on different computational units, and later discarded immediatelly, since a read-only buffer is assumed to remain unchanged at the end of a task.

\itodo{diagrama a exemplificar esta coisa. Tasks A,B,C. tasks A,B have read-only access to x. Task C has write access. If tasks are in the order A,B,C, then A,B can be ran concurrently. If C is between, then the order must be A,C,B each at a time}


\section{Virtual Shared Memory}
\itodo{this}

The approach used by \starpu when it comes to memory management is simpler that the model employed by \gama. The purpose is the same: to automatically manage memory allocations and transfers on all devices. This not only frees the programmer from the work of manually managing memory between tasks, but it also has the potential to lower the cost of such operations.
\starpu manages memory by forcing the user to declare data handles to their data. This handles are used as arguments for tasks, allowing the scheduler to allocate and transfer all required data buffers to the correct computational unit prior to the task execution.

\itodo{nas conclus√µes: caveats}

\section{Multithreading} \label{section:starpu_multithreading}
\itodo{this}

In order to have an accurate view of the topology of the system, the framework can optionally use the \texttt{hwloc} \cite{broquedis2010hwloc} to detect the structure of the architecture, including all \cpu sockets, \acs{NUMA} nodes and cache hierarchy.

A tree is created representing the complete hierarchy of the system. Latest version of the framework also introduce support for parallel tasks, with the concept of combined workers. These workers exist in cases where the system has multiple computational units in the same node (such as \cpu sockets where multiple cores share a cache hierarchy). In these situations, and if \starpu is using a parallel-task-aware scheduler (currently only \texttt{pheft} and \texttt{peager} exist), it is possible to specify the maximum degree of parallelism for a function. A combined worker can then be assigned to such tasks using, for example, \openmp to take advantage of the multiple available cores.

\section{API} \label{section:starpu_api}
\itodo{this}

Two API versions are made available. The high-level, \textt{pragma}-based\footnote{Directives inserted within the code, to instruct the compiler about how to process input. Can be used to extend the compiler and provide additional features, functionality or optimizations} API. This exposes \starpu's main functionality with a set of directives that can be added to the code in order to embed \starpu within it. It is particularly suited for less experienced developers, or developers who simply need to focus completely in the algorithmic issues with less or no knowledge about the underlying parallel hardware being used underneath.

The directives can be easily disabled, restoring the application to its original, \starpu-free nature, which also makes it ideal to add \starpu into already existing code, or applications that require a large degree of portability.

The low-level version is a more verbose one, which is actually used internally by the high-level one, and provides a greater degree of control over what \starpu does.

This trade-off is not uncommon, with many existing libraries besides \starpu supporting both API levels. High level versions are designed to remove complexity and accelerate development time. They are often a subset of an underlying low level version, delivering only the more common features. More experienced developers should be able to achieve better results with a lower level API, with the cost of additional development time.

\section{Performance Model}

\end{document}


