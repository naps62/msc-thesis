\documentclass[main.tex]{subfiles}

\begin{document}
\section{Performance Results With \starpu}

In order to accurately profile the performance obtained by using the framework
\itodo{falta aqui texto}

\subsection{Scheduler Impact}

\cref{fig:prof:starpu_cpu} shows the overhead of using the framework to schedule tasks instead of directly invoking them. Execution times were measured with different schedulers, with only \cpu tasks, and compared against the best \cpu times obtained without the framework. No major speedups are expected here, since the framework should itself create a significantly high overhead, but it should be interesting to see if delegating the task of choosing \openmp thread pool size to \starpu, rather than manually tuning it, directly impacts performance.

This was mostly a concern due to the scalability problems observed in \cref{sec:prof:cpu}. If \starpu, using one of the less smart schedulers, would opt to eagerly use all available \cpus, performance would degrade as seen in \cref{fig:prof:cpu}. This was indeed the observed result with the \textbf{peager} scheduler. \textbf{dm} and \textbf{dmda} also degrade performance down to around 4x (which roughly corresponds to worst-case scenarios observed on \cpu implementation), but that is to be expected since these schedulers do not support parallel tasks. Since no accelerators are being used here, this results in all tasks being ran sequentially.

\itodo{proenca: fazer a comparacao do dm e dmda com a versao cpu\_1?}

\itodo{neste grafico, mudar a escala para 0..1,2 com step 0.2}

\image[width=0.6\textwidth]{profiling/starpu_cpu_scheds}{\starpu implementation, \cpu-only}{fig:prof:starpu_cpu}

\subsection{Performance with accelerators}

To test how \cuda devices influenced the algorithm, measurements were made for both individual \cuda devices, and with both available \gpus for \starpu to use.
When using accelerators, all schedulers are able to speedup the implementation when compared to the best \cpu times (which were achieved with around 6 threads). However, as seen in \cref{fig:prof:starpu_scheds}, the gain difference between each scheduler is noticeable: \textbf{dmda}, which performs poorly on \cpu due to sequentializing tasks sees the largest improvement. This is expected since it is essentially a comparison between sequential \cpu tasks with massively parallel \cuda tasks. The fact to note here is that \textbf{dm} has much worse evolution under the same conditions.

This enforces the fact that memory transfers are extremely important to take into account by the scheduler, as this is the only difference between the two.

As for \textbf{peager} and \textbf{pheft}, their gains are not as large, since the \cpu code was already parallelized, but it is relevant to note that the smarter \textbf{pheft} seems to be outperformed once the whole set of devices is used. This is a consequence of the low iteration level-parallelism available. Since each tasks is fully scheduled to a given device, and multiple task dependencies exist within an iteration, it is difficult to efficiently take advantage of multiple devices simultaneously, in which case eagerly selecting the best device can be considered a faster and more efficient choice.

Unfortunately, when using \textbf{pheft} with the Fermi device, memory errors would constantly be raised, so it was not possible to finish those tests successfully. This is most likely due to problems with this particular scheduler, which is still under development by the \starpu team, and thus cannot be assumed to be fully functional.

\begin{figure}[!htp]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{profiling/starpu_sched_peager}
    \caption{peager \label{fig:prof:starpu_sched_peager}}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{profiling/starpu_sched_pheft}
    \caption{pheft \label{fig:prof:starpu_sched_pheft}}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{profiling/starpu_sched_dm}
    \caption{dm \label{fig:prof:starpu_sched_dm}}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{profiling/starpu_sched_dmda}
    \caption{dmda \label{fig:prof:starpu_sched_dmda}}
  \end{subfigure}
  \caption{Avg. iteration time of the different schedulers with \gpu devices \label{fig:prof:starpu_scheds}}
\end{figure}

One of the most important aspects to analyse was how the chosen scheduler impacts performance.

\subsection{Overall Performance Comparison}

The best case scenario for each possible approach is shown in \cref{fig:prof:overall}. This serves to show the impact of \starpu with each different scheduler against framework-less solutions.

\begin{figure}[!htp]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{profiling/1iter_time}
    \caption{Avg. iteration time \label{fig:prof:overall_time}}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{profiling/1iter_speedup}
    \caption{Speedup \label{fig:prof:overall_speedup}}
  \end{subfigure}
  \caption{Best cases for each different implementation and scheduler \label{fig:prof:overall}}
\end{figure}

\begin{table}[!htb]
  \begin{tabular}{|r|rrr|rrrr|}
    \hline
            & Sequential & \cpu & \cuda & \textbf{peager} & \textbf{pheft} & \textbf{dm} & \textbf{dmda} \\ \hline
    cornell & 3.03       & 1.65 & 1.11  & 1.51            & 1.33           & 2.42        & 1.12 \\
    kitchen & 3.46       & 1.02 & 1.15  & 1.46            & 0.96           & 2.75        & 1.12 \\
    luxball & 5.91       & 2.35 & 1.76  & 2.15            & 2.06           & 4.55        & 1.71 \\
    \hline
  \end{tabular}
  \caption{Avg Iteration time for all versions \label{tab:overall_time}}
\end{table}

\begin{table}[!htb]
  \begin{tabular}{|r|rr|rrrr|}
    \hline
            & \cpu & \cuda & \textbf{peager} & \textbf{pheft} & \textbf{dm} & \textbf{dmda} \\ \hline
    cornell & 1.83 & 2.73  & 2.01            & 2.27           & 1.25        & 2.71 \\
    kitchen & 3.39 & 2.30  & 2.36            & 3.60           & 1.26        & 3.09 \\
    luxball & 2.52 & 3.36  & 2.75            & 2.87           & 1.30        & 3.45 \\
    \hline
  \end{tabular}
  \caption{Avg speedup for all versions \label{tab:overall_speedup}}
\end{table}

\subsection{Concurrent Iterations}

With the employed approach, task-level parallelism is limited. The \textbf{dmda} does not support combined workers, greatly lowering efficiency of \cpu tasks. \textbf{pheft} does support this, but its not a data aware scheduler, meaning that data transfers are not considered when assigning tasks. As a result, performance with \starpu is limited when using processing a single iteration.

\image[width=0.6\textwidth]{profiling/iters_at_once_both_speedup}{Speedup with concurrent iterations}{fig:prof:iters_at_once_both_speedup}

By using concurrent iterations, however, extra speedups can be achieved, as shown in \cref{fig:prof:iters_at_once_both_speedup}.

\itodo{proença: este capitulo está pouco claro e pobre}

\end{document}
